Representing brain structures using three-dimensional Cartesian coordinates (x, y, z) provides a foundational framework for describing spatial relationships in the brain. Stereotactic coordinate spaces, such as the Montreal Neurological Institute (MNI) template \cite{Avants2008-ek,Fonov2009-oi}, serve as common reference systems to align brain images, enabling comparisons between subjects and large-scale studies. This spatial standardization supported automated meta-analytic tools like NeuroSynth \cite{Yarkoni2011-sr} and NeuroQuery \cite{Dockes2020-nw}, which aggregate reported coordinates from thousands of studies to identify consistent brain–behavior associations. Shared coordinate systems allowed the neuroimaging community to build reproducible and interpretable models of the brain's structure and function at the population level.
Coordinate-based mapping is also central in clinical neuroscience. Stereotactic neurosurgery relies on spatial reference systems to plan interventions, often using landmarks such as the anterior and posterior commissure line (AC-PC) to define individualized coordinate spaces \cite{Talalrach1957-bs}. Precise brain target localization is particularly important in procedures like deep brain stimulation (DBS), where millimetric deviations in electrode placement can influence therapeutic outcomes and side-effect profiles \cite{Horn2018-qq}. Software platforms such as Lead-DBS \cite{Neudorfer2023-wd} have further standardized the mapping and visualization of DBS electrodes in stereotactic coordinate spaces, supporting cross-center comparisons and data-driven approaches to target refinement \cite{Ewert2018-bn}.


Stereotactic spaces provide a structured framework for describing spatial relationships in the brain. By mapping individual anatomy into a shared reference space, these systems enable localization of surgical targets and support reproducible planning and analysis. Stereotactic procedures rely on such systems to translate anatomical knowledge into actionable spatial coordinates. However, their accuracy depends on the quality of image acquisition, the fidelity of image alignment, and the resolution at which anatomical structures are represented.


\subsection{Opportunities for Fiducial-Guided Learning}

This dissertation explores the potential of anatomical fiducials—visually identifiable, reproducible landmarks—as spatially grounded anchors for ML workflows in neuroimaging and stereotactic targeting. Fiducials offer millimetric precision, anatomical interpretability, and compatibility with multimodal imaging, making them ideal candidates for training or supervising models that require spatial accuracy. For example, fiducial-based coordinate regression can be used to localize invisible or indirect surgical targets (AFIDPred), while automatically detected fiducials (Auto-AFIDs) can support robust, contrast-agnostic landmark localization across heterogeneous datasets.

Fiducials also serve as a natural interface for evaluating image registration quality (AFIDQC) and for constructing population-level morphometric models that preserve anatomical meaning (AFIDCharts). By embedding ML models within a fiducial-defined coordinate space, this approach balances the scalability of automation with the transparency of anatomically grounded supervision. Unlike many ML pipelines that rely on opaque features or latent representations, fiducial-guided learning promotes interpretability, reproducibility, and integration with clinical and research workflows.

\section{Machine Learning in Neuroimaging}
The field of neuroimaging has been transformed by the convergence of two powerful trends: (1) the availability of large-scale imaging datasets and (2) advances in computational methods. Public repositories such as the Human Connectome Project [ref], UK Biobank [ref], and Alzheimer's Disease Neuroimaging Initiative [ref] facilitated large-scale studies of brain structure, function, and disease across diverse populations. At the same time, improvements in computational resources, particularly the widespread use of open-source machine learning (ML) frameworks, made it possible to apply sophisticated algorithms to neuroimaging data at scale. These developments have fueled the rapid adoption of ML in both research and clinical settings, where models are now used to automate complex tasks such as tissue segmentation [ref], image registration [ref], and disease classification [ref]. In clinical applications, ML models are increasingly being explored for diagnostic decision support [ref] and surgical planning [ref]. However, as ML models grow in complexity, their integration into clinical workflows, like stereotactic neurosurgery, requires careful attention to anatomical accuracy, spatial interpretability, and reproducibility to ensure both scientific validity and patient safety. Talk abit about reinfocement learning and then mention something like these two are beyond the scope of this dissertation.  
\subsection{Overview of Machine Learning}
Machine learning (ML) refers to a class of computational techniques that enable algorithms to identify patterns in data and make predictions or decisions without being explicitly programmed for each specific task. At its core, ML involves training a model using a dataset that includes both input features (e.g., brain images, anatomical measurements) and known outputs (e.g., disease status, anatomical labels). The model iteratively adjusts internal parameters to minimize the error between its predictions and the known outputs. Once trained, the model can then be applied to new, unseen data to generate predictions.
ML methods can broadly be categorized into supervised, unsupervised, and reinforcement learning approaches. In neuroimaging, supervised learning is most commonly used, where models are trained on labeled datasets to perform tasks such as segmentation (assigning labels to brain regions), classification (e.g., distinguishing Alzheimer’s disease from controls), or regression (e.g., predicting age or symptom severity). Unsupervised learning, on the other hand, aims to uncover hidden structure in unlabeled data, such as clustering patients with similar brain patterns or discovering latent brain networks.
\subsection{Traditional Machine Learning Models}
Traditional machine learning approaches in neuroimaging typically rely on handcrafted features—quantitative summary measures extracted from preprocessed scans using domain expertise and predefined anatomical or functional templates. Common examples include regional brain volumes derived from segmentation protocols (e.g., hippocampal volume in Alzheimer’s disease), cortical thickness measurements from surface reconstructions (e.g., via FreeSurfer), white matter integrity metrics from diffusion tensor imaging (e.g., fractional anisotropy), and voxel-wise signal intensities from structural or functional scans. These features reduce the dimensionality of raw imaging data and make it more tractable for classical algorithms.

Once extracted, these features are input into supervised learning algorithms such as support vector machines (SVMs), random forests, logistic regression, and ridge regression, each of which aims to map imaging-derived features to clinical or anatomical outcomes. Ridge regression, a linear model with L2 regularization, is often used when the number of features is large and potentially collinear—as is common in neuroimaging. It helps prevent overfitting by penalizing large coefficient values, improving model stability and generalizability. In contrast, Extreme Gradient Boosting (XGBoost) is a more advanced, ensemble-based method that builds a series of decision trees sequentially, where each tree attempts to correct the errors of its predecessor. XGBoost is capable of capturing complex nonlinear relationships and interactions between features, making it well-suited for high-dimensional data with subtle patterns.

These models are particularly appealing due to their interpretability, regularization options, and relatively low computational cost. For example, ridge regression has been used to predict clinical scores such as MMSE or UPDRS based on regional morphometric data, while XGBoost has shown success in classifying neurological conditions like Parkinson’s disease or schizophrenia based on multimodal imaging inputs.

However, the performance of these models remains highly dependent on the quality and relevance of the chosen features. Handcrafted features may fail to capture distributed or hierarchical patterns in the data, and models like ridge regression, while robust, assume linear relationships between inputs and outputs. Even flexible models like XGBoost, despite their ability to model nonlinearity, are only as powerful as the input features allow. Additionally, handcrafted features often represent region-level averages, which obscure local heterogeneity and ignore fine-grained spatial dependencies.

These limitations underscore the fact that traditional ML approaches, while foundational, are constrained by their indirect use of image data, their limited spatial expressiveness, and their dependence on predefined representations. These challenges have motivated the adoption of deep learning techniques, which seek to learn hierarchical and spatially grounded representations directly from raw neuroimaging data.

\subsection{Deep Learning Models}
While traditional machine learning methods rely on manually extracted features and structured input data, deep learning (DL) has enabled a new class of models that can learn directly from raw neuroimaging data. DL, a subfield of machine learning, uses artificial neural networks with multiple layers to automatically extract, combine, and abstract complex representations of data. This capability has made DL particularly well-suited for image-based tasks in neuroimaging, where spatial hierarchies and subtle structural variations carry significant information.

Among DL architectures, convolutional neural networks (CNNs) have emerged as the dominant approach for analyzing medical images. CNNs are composed of interconnected layers that apply learnable spatial filters to small image regions, capturing localized features such as edges, textures, and patterns. As information passes through deeper layers of the network, these features are combined into increasingly abstract representations, allowing the model to recognize complex anatomical structures or pathological changes.

In neuroimaging, CNNs have demonstrated strong performance across a variety of tasks, including:

Brain tissue segmentation, where models differentiate between gray matter, white matter, and cerebrospinal fluid

Tumor detection and classification, especially in glioma and metastasis cases

Structural parcellation, identifying cortical and subcortical regions

Functional activation decoding, such as mapping fMRI signals to cognitive tasks or mental states

A particularly influential architecture in biomedical imaging is the U-Net, originally introduced by Ronneberger et al. (2015) for biomedical image segmentation. U-Net follows an encoder–decoder architecture, designed to balance contextual understanding with precise localization. The encoder path consists of a series of convolutional and pooling layers that progressively reduce spatial resolution while extracting increasingly abstract semantic features. The decoder path uses transposed convolutions to upsample these representations and recover the original spatial dimensions. Crucially, skip connections link each encoder layer with its corresponding decoder layer, allowing the model to retain fine-grained spatial details that would otherwise be lost during downsampling.

This design enables U-Nets to achieve high segmentation accuracy, even when trained on relatively small datasets—a major advantage in medical imaging where labeled data is limited. The network is typically trained using pixel-wise loss functions such as cross-entropy or Dice loss, which optimize the accuracy of each pixel’s predicted class. In neuroimaging, U-Nets and their 3D variants (e.g., 3D U-Net, V-Net) have become the architecture of choice for tasks such as skull stripping, white matter hyperintensity detection, tumor segmentation, and subcortical parcellation. Extensions of the U-Net architecture continue to evolve, incorporating residual connections, attention mechanisms, and multi-scale feature fusion to further improve performance in low-contrast or anatomically ambiguous regions.

Despite these strengths, deep learning models—U-Nets included—pose challenges for clinical integration. They operate largely as black boxes, making their decision-making processes difficult to interpret. This lack of transparency is problematic in clinical settings where understanding the basis of a prediction is essential for safety, trust, and accountability. Moreover, DL models are highly sensitive to domain shifts, such as changes in imaging modality, scanner vendor, or patient population, and often require retraining or domain adaptation to generalize effectively.

These challenges are particularly pronounced in stereotactic neurosurgery, where even millimetric localization errors can have significant consequences for patient outcomes. In such high-stakes contexts, the inability of DL models to guarantee spatial fidelity or anatomical consistency limits their clinical applicability. While they may achieve impressive performance metrics, these models do not inherently encode anatomical priors or adhere to known spatial relationships—raising concerns when predictions deviate from expected anatomical norms.

As such, there is a growing recognition that in settings where spatial accuracy and interpretability are paramount, performance alone is insufficient. There is a need for machine learning frameworks that are anatomically constrained, human-interpretable, and robust to data variability. This motivates the use of fiducial-based approaches, which incorporate consistently identifiable anatomical landmarks into ML models. By grounding predictions in known coordinate systems, fiducial-guided frameworks offer a promising path toward accurate, reproducible, and clinically meaningful deep learning in neuroimaging.

\subsection{Applications in Segmentation, Registration, and Target Localization}

One of the most mature applications of deep learning in neuroimaging is anatomical segmentation. CNN-based models have been used to delineate brain tissues, subcortical nuclei, lesions, and tumors with accuracy comparable to expert human raters \cite{dolz2018denseunet}. For instance, networks trained on high-resolution T1-weighted images can reliably segment regions like the hippocampus or thalamus, which are critical in neurodegenerative disease research and surgical planning.

Image registration—the alignment of images across time, modality, or subject—is another key domain of ML application. Recent methods such as VoxelMorph have demonstrated that deep networks can learn deformation fields for non-linear registration in an unsupervised fashion, often achieving results comparable to classical algorithms while dramatically reducing computation time \cite{balakrishnan2019voxelmorph}. Registration accuracy, however, remains sensitive to contrast, anatomical variability, and initialization, all of which impact downstream tasks such as morphometry and surgical planning.

ML is also being used to localize brain targets that are difficult to visualize directly. In the context of deep brain stimulation (DBS), for example, researchers have trained models to identify target locations (e.g., subthalamic nucleus, globus pallidus internus) based on multimodal features or indirect anatomical markers \cite{yang2020deep}. These approaches promise to improve standardization and reduce inter-rater variability in surgical targeting, though they often depend on well-curated training datasets and may be vulnerable to domain shift.

\subsection{Challenges in Generalization and Interpretability}

Despite their promise, ML and DL models face significant challenges in clinical neuroimaging. A major concern is generalization: models trained on data from a specific scanner, protocol, or population may not perform well when deployed in a different setting \cite{zech2018variable}. Neuroimaging datasets are notoriously heterogeneous, and even subtle differences in acquisition parameters can introduce bias. Efforts such as domain adaptation, harmonization, and transfer learning attempt to mitigate these effects, but the problem remains a barrier to real-world deployment.

Interpretability is another limiting factor, particularly in high-stakes applications such as neurosurgical planning. Many deep learning models function as ``black boxes,'' providing predictions without a clear explanation of their basis. This opacity can undermine clinician trust and complicate error analysis. Techniques such as saliency mapping, attention mechanisms, and feature attribution have been proposed to improve interpretability, but these methods often lack the spatial grounding necessary for anatomically meaningful insights \cite{holzinger2019causability}.

Moreover, most ML pipelines are trained and evaluated without strong anatomical priors, making them susceptible to anatomically implausible outputs—particularly in cases of registration error or abnormal anatomy. There is a growing consensus that embedding domain knowledge and spatial constraints into ML systems could improve both robustness and transparency.



\section{Brain Coordinates}
Representing brain structures using three-dimensional Cartesian coordinates (x, y, z) provides a foundational framework to describe spatial relationships in the brain. 

\subsection{Concept and Rationale for Fiducials}
\subsection{The Anatomical Fiducials (AFIDs) Protocol}
\subsection{Strengths of Fiducial-Based Approaches}


\subsection{Multimodal MRI}
Multimodal MRI extends the utility of conventional structural imaging by integrating additional contrast mechanisms and tissue properties to improve the localization of surgical targets.

Diffusion-weighted imaging (DWI) and diffusion tensor imaging (DTI) provide estimates of white matter tract orientation by characterizing the anisotropic diffusion of water molecules. These sequences allow for patient-specific tractography, which can be overlaid on structural images to support targeting of subregions within the subthalamic nucleus (STN) or globus pallidus internus (GPi) that are most strongly connected to motor cortex. This has led to the emergence of connectivity-informed DBS targeting strategies.

Susceptibility-weighted imaging (SWI) and quantitative susceptibility mapping (QSM) exploit magnetic field perturbations caused by local iron content, offering improved contrast in iron-rich nuclei such as the STN, red nucleus, and substantia nigra. These sequences are particularly useful for differentiating closely packed subcortical structures that may appear indistinct on T1- or T2-weighted scans.

Together, multimodal MRI improves boundary definition, supports registration accuracy, and enables more robust identification of individualized DBS targets—particularly in anatomically ambiguous or distorted cases.

\subsection{Ultra-high Field MRI}
Ultra-high field (UHF) MRI, typically defined as systems operating at 7 Tesla and above, offers substantial gains in signal-to-noise ratio (SNR), spatial resolution, and contrast-to-noise ratio (CNR). These gains are particularly valuable for the visualization of small and densely packed subcortical structures that are central to functional neurosurgery.

At higher field strengths, the Larmor frequency increases, leading to greater net magnetization and improved sensitivity to subtle tissue differences. This enables higher-resolution imaging, often with voxel sizes approaching 0.5 mm\textsuperscript{3}, which dramatically increases the number of voxels spanning small nuclei. For example, at 0.5 mm isotropic resolution, the number of voxels covering the STN increases nearly eightfold compared to standard 1 mm resolution, allowing for clearer delineation of functional subregions.

Moreover, UHF imaging facilitates in vivo visualization of structures previously accessible only through histology, such as the caudal zona incerta (cZI) and fields of Forel. These regions have re-emerged as viable DBS or lesion targets, especially for tremor and dystonia, but are difficult to localize using standard imaging methods.

However, UHF MRI introduces technical challenges including increased magnetic susceptibility artifacts, \( B_0 \) and \( B_1 \) inhomogeneities, and higher specific absorption rate (SAR), all of which must be mitigated through advanced acquisition strategies, coil designs, and RF calibration methods. While clinical adoption remains limited, ongoing improvements in hardware and pulse sequence development continue to enhance the utility of UHF MRI in functional neurosurgery.


To Jon — thank you for creating a research environment where curiosity and compassion coexisted seamlessly. Your meticulous approach to science, your unwavering dedication to your students, and your deep care for patients set a standard I will always aspire to. You gave me the freedom to explore, the feedback to refine, and the mentorship to grow into an independent scientist. Watching you translate research into surgical precision in the operating room was not only inspiring, but helped me anchor my own work in the realities and hopes of clinical care. You taught me that excellence lies in both the details and in the greater purpose behind our work.

To Ali — thank you for showing me that there is more than one path to sound scientific discovery. Your encouragement to integrate programming into my research transformed not only my toolkit but my confidence as a computational scientist. Your commitment to openness — in software, in collaboration, and in thinking — gave visibility and vitality to this work. Through your example, I learned that innovation grows best in cultures that value sharing over silos.

Together, Jon and Ali fostered a lab culture that brought out the best in its students — one where ideas were exchanged freely, support was abundant, and the pursuit of knowledge was both rigorous and joyful. I am deeply grateful to have been part of that.










\section{Image Registration}
\label{sec:registration}
Image registration is a fundamental component of neuroimaging analysis, facilitating spatial alignment of imaging volumes either over time, over modalities (e.g. MRI and CT) or to stereotactic spaces such as the MNI space. It underpins both clinical and research applications, including intraoperative navigation, longitudinal monitoring, and multimodal image fusion. In DBS, registration plays a critical role in maintaining spatial continuity across the surgical workflow. Accurate coregistration ensures that anatomical information derived from preoperative MRI can be reliably transferred to stereotactic planning coordinates and subsequently reconciled with postoperative imaging for DBS electrode localization and verification.

\subsection{Image Registration Fundamentals}
In its most general form, registration aims to estimate a transformation \( v: \Omega_T \rightarrow \Omega_S \) that maps coordinates from a fixed image domain \( \Omega_T \) to a moving image domain \( \Omega_S \), such that the transformed moving image \( \tilde{S}(x) = S(v(x)) \) becomes aligned with the fixed image \( T(x) \). This process can be formulated as the following optimization problem:
\begin{equation}
    \arg\min_{v} \, \mathcal{C}(S \circ v, T) + \lambda \mathcal{R}(v)
    \label{eq:registration_loss}
\end{equation}
where \( \mathcal{C} \) is a similarity metric (data fidelity term), \( \mathcal{R} \) is a regularization term enforcing smoothness or anatomical plausibility, and \( \lambda \) balances the two.
\paragraph{Transformation Models.}
Registration transformations are typically categorized based on the degrees of freedom (DoF) they allow:

\begin{itemize}
    \item \textbf{Rigid} transformations include only translation and rotation (6 DoF), preserving distances and angles.
    \item \textbf{Affine} transformations extend rigid by adding scaling and shearing (12 DoF). They are represented as:
    \begin{equation}
        v(x) = \mathbf{M}x + t
        \label{eq:affine}
    \end{equation}
    where \( \mathbf{M} \in \mathbb{R}^{3 \times 3} \) captures linear effects, and \( t \in \mathbb{R}^3 \) is a translation vector.
    
    \item \textbf{Nonlinear (Deformable)} transformations introduce spatially varying displacements:
    \begin{equation}
        v(x) = x + u(x)
        \label{eq:deformable}
    \end{equation}
    where \( u(x) \) is a smooth displacement field. 
    
\end{itemize}

\paragraph{Similarity Metrics.}
To quantify alignment quality between the fixed image \( T(x) \) and the transformed moving image \( \tilde{S}(x) \), various similarity metrics are employed depending on the imaging context:
\begin{itemize}
    \item \textbf{Sum of Squared Differences (SSD)} is appropriate for intra-modality registration:
    \begin{equation}
        \mathcal{C}_{\text{SSD}}(T, \tilde{S}) = \sum_{x \in \Omega_T} \left( T(x) - \tilde{S}(x) \right)^2
        \label{eq:ssd}
    \end{equation}
    \item \textbf{Normalized Cross-Correlation (NCC)} accounts for global intensity shifts:
    \begin{equation}
        \mathcal{C}_{\text{NCC}}(T, \tilde{S}) = \frac{\sum_x (T(x) - \mu_T)(\tilde{S}(x) - \mu_{\tilde{S}})}{\|T - \mu_T\| \cdot \|\tilde{S} - \mu_{\tilde{S}}\|}
        \label{eq:ncc}
    \end{equation}
    \item \textbf{Mutual Information (MI)} is robust to cross-modality differences:
    \begin{equation}
        \mathcal{C}_{\text{MI}}(T, \tilde{S}) = - \left[ H(T) + H(\tilde{S}) - H(T, \tilde{S}) \right]
        \label{eq:mi}
    \end{equation}
    where \( H(\cdot) \) denotes the Shannon entropy. MI is widely used in multimodal scenarios such as MRI–CT registration.
\end{itemize}

\paragraph{Optimization and Regularization.}
Optimization strategies include multiresolution gradient descent, Powell methods, or stochastic algorithms, and are chosen based on the complexity of images and the metric used. Regularization terms \( \mathcal{R}(v) \) ensure that transformations are smooth and physically meaningful, especially in deformable registration, where large local variations could lead to implausible warps.

\subsection{Brain Templates and Atlases}
\label{sec:atlases_templates}
The first stereotactic neurosurgical procedures were guided by knowledge from cadaveric specimens and sections. Atlases such as those by Talairach and Tournoux \cite{Talairach1957-eb,Talairach1988-wk} or Schaltenbrand-Wahren \cite{Schaltenbrand1977-ge} provided sectional views of the brain with overlaid coordinate systems anchored to internal features like the AC and PC line. These atlases were designed to support proportional scaling, allowing neurosurgeons to infer the location of deep brain structures relative to patient anatomy using intraoperative imaging modalities such as ventriculography or angiography. Through these frameworks, atlas-based targeting became a central feature of stereotactic planning.

The advent of digital neuroimaging transformed atlas-based targeting. With the introduction of MRI and CT, and later population-averaged anatomical datasets, the field shifted from fixed, printed atlases toward deformable, individualized imaging. This shift introduced a useful conceptual distinction: a \textbf{template} is now understood as a coordinate-defined brain volume—often an average of many individuals—that serves as a registration target for aligning individual images. Examples include the MNI templates, which define modern stereotactic spaces.

An \textbf{atlas}, by contrast, is now commonly defined as an annotation or labeling schema superimposed onto a template. These can include deterministic labels (e.g., segmentations of the subthalamic nucleus) or probabilistic maps reflecting anatomical variability. Examples include the ? In practice, a subject's MRI is first registered to a template via rigid or nonlinear transformation; only then can atlas labels be transferred and interpreted in the subject's anatomical context.

Stereotactic spaces (e.g., MNI templates \cite{Avants2008-ek,Fonov2009-oi}), serve as common reference systems to align brain images, enabling comparisons between subjects and large-scale studies. This spatial standardization supported automated meta-analytic tools like NeuroSynth \cite{Yarkoni2011-sr} and NeuroQuery \cite{Dockes2020-nw}, which aggregate reported coordinates from thousands of studies to identify consistent brain–behavior associations. Stereotactic spaces allowed the neuroimaging community to build reproducible and interpretable models of the brain's structure and function at the population level.

In DBS, stereotactic atlases serve two main goals: 1) during target localization and planning to visualize conspicuous targets [ref] and 2) registration to a population-based template is employed by the majority of studies that attempt to understand the effect on targeted neural circuits [ref]. 

enables normalization across subjects, allowing for the application of probabilistic atlases such as DISTAL \cite{Ewert2018} or CIT168 \cite{Pauli2018}. However, the accuracy of this process depends not only on the fidelity of the atlas, but on the precision of the image registration step. Misalignment—even on the order of 1–2 mm—can propagate through surgical planning and impact electrode localization. These concerns motivate the need for rigorous validation strategies and ground-truth frameworks, such as anatomical fiducials, to quantify correspondence between subject data and reference spaces.

\subsection{Registration Tools and Pipelines}
Several image registration toolkits and workflows are widely used in stereotactic applications. Commonly used software packages include ANTs (Advanced Normalization Tools), FSL FLIRT, SPM, and Elastix, each providing implementations for rigid, affine, and non-linear registration algorithms.

In the context of DBS planning, image registration pipelines often follow a multi-stage approach. For example, a rigid transformation may be used to align the post-frame CT scan to the preoperative T1-weighted MRI. This is followed by affine or non-linear registration of the subject's MRI to a standard template such as MNI space. At each step, care must be taken to minimize distortion and maintain anatomical fidelity, especially when registering across modalities. To address resolution mismatches or voxel anisotropy, interpolation techniques such as trilinear, spline, or sinc interpolation are used to resample the moving image.


\subsection{Applications in Deep Brain Stimulation}
Accurate registration is critical in DBS for preoperative planning, intraoperative targeting, and postoperative lead localization. During planning, MRI-based target coordinates must be accurately transferred to the stereotactic frame space defined by an intraoperative CT. This enables trajectory planning, simulation of lead placement, and risk assessment relative to nearby structures.

Postoperatively, registration is used to localize the implanted DBS lead within the patient's MRI or a template brain. This localization enables group-level comparisons of stimulation loci, construction of activation volume models, and retrospective analyses of clinical efficacy. Lead-DBS, a widely adopted platform for DBS analysis, relies on multi-stage registration pipelines that include linear and non-linear warps to align patient data to MNI space.

Atlas-based targeting, another application of registration, leverages deformable transformations to map subject images to high-resolution histological atlases. This allows for the identification of subregions within a target nucleus (e.g., sensorimotor STN), improving both precision and outcome predictability.

\subsection{Limitations in Quality Control}
Despite its importance, image registration remains a potential source of error in stereotactic workflows. Registration accuracy is influenced by multiple factors, including voxel size, image contrast, presence of artifacts, and interpolation method. Importantly, most automated tools provide limited insight into registration quality, making visual inspection and validation essential.

Errors may arise due to intensity non-uniformity, particularly in MR images acquired at ultra-high field strengths, or due to misregistration across modalities with differing tissue contrast. Mutual information, while robust for multimodal registration, may produce anatomically implausible results if poorly initialized. Deformable registration, although powerful, may overfit to noise or distort anatomy if not properly regularized.

Recent developments in machine learning have introduced data-driven registration models that bypass iterative optimization by directly predicting deformation fields. While promising in speed and consistency, these approaches are not yet standard in clinical stereotaxy due to limited interpretability and generalizability.

In clinical practice, registration quality is typically assessed by overlaying anatomical landmarks or fiducial markers and inspecting alignment. Automated quality control metrics, such as target registration error (TRE) or landmark distance, can support this process but are rarely integrated into surgical pipelines. Establishing standardized, quantitative registration QC tools remains an unmet need in functional neurosurgery.



------

While traditional ML methods rely on manually extracted features and structured input data, deep learning (DL) has enabled a new class of models that can learn directly from raw neuroimaging data \cite{Sarker2021-fo}. DL, a subfield of machine learning, uses artificial neural networks with multiple layers to automatically extract, combine, and abstract complex representations of data. This capability has made DL particularly well-suited for image-based tasks in neuroimaging, where spatial hierarchies and subtle structural variations carry significant information.

Among DL architectures, convolutional neural networks (CNNs) have emerged as the dominant approach for analyzing medical images. CNNs are composed of interconnected layers that apply learnable spatial filters to small image regions, capturing localized features such as edges, textures, and patterns. As information passes through deeper layers of the network, these features are combined into increasingly abstract representations, allowing the model to recognize complex anatomical structures or pathological changes.

In neuroimaging, CNNs have demonstrated strong performance across a variety of tasks, including:

Brain tissue segmentation, where models differentiate between gray matter, white matter, and cerebrospinal fluid

Tumor detection and classification, especially in glioma and metastasis cases

Structural parcellation, identifying cortical and subcortical regions

Functional activation decoding, such as mapping fMRI signals to cognitive tasks or mental states

A particularly influential architecture in biomedical imaging is the U-Net, originally introduced by Ronneberger et al. (2015) for biomedical image segmentation. U-Net follows an encoder–decoder architecture, designed to balance contextual understanding with precise localization. The encoder path consists of a series of convolutional and pooling layers that progressively reduce spatial resolution while extracting increasingly abstract semantic features. The decoder path uses transposed convolutions to upsample these representations and recover the original spatial dimensions. Crucially, skip connections link each encoder layer with its corresponding decoder layer, allowing the model to retain fine-grained spatial details that would otherwise be lost during downsampling.

This design enables U-Nets to achieve high segmentation accuracy, even when trained on relatively small datasets-a major advantage in medical imaging where labeled data is limited. The network is typically trained using pixel-wise loss functions such as cross-entropy or Dice loss, which optimize the accuracy of each pixel’s predicted class. In neuroimaging, U-Nets and their 3D variants (e.g., 3D U-Net, V-Net) have become the architecture of choice for tasks such as skull stripping, white matter hyperintensity detection, tumor segmentation, and subcortical parcellation. Extensions of the U-Net architecture continue to evolve, incorporating residual connections, attention mechanisms, and multi-scale feature fusion to further improve performance in low-contrast or anatomically ambiguous regions.

Despite these strengths, deep learning models-U-Nets included-pose challenges for clinical integration. They operate largely as black boxes, making their decision-making processes difficult to interpret. This lack of transparency is problematic in clinical settings where understanding the basis of a prediction is essential for safety, trust, and accountability. Moreover, DL models are highly sensitive to domain shifts, such as changes in imaging modality, scanner vendor, or patient population, and often require retraining or domain adaptation to generalize effectively.

These challenges are particularly pronounced in stereotactic neurosurgery, where even millimetric localization errors can have significant consequences for patient outcomes. In such high-stakes contexts, the inability of DL models to guarantee spatial fidelity or anatomical consistency limits their clinical applicability. While they may achieve impressive performance metrics, these models do not inherently encode anatomical priors or adhere to known spatial relationships-raising concerns when predictions deviate from expected anatomical norms.

As such, there is a growing recognition that in settings where spatial accuracy and interpretability are paramount, performance alone is insufficient. There is a need for machine learning frameworks that are anatomically constrained, human-interpretable, and robust to data variability. This motivates the use of fiducial-based approaches, which incorporate consistently identifiable anatomical landmarks into ML models. By grounding predictions in known coordinate systems, fiducial-guided frameworks offer a promising path toward accurate, reproducible, and clinically meaningful deep learning in neuroimaging.

\subsection{Current Applications}
One of the most mature applications of deep learning in neuroimaging is anatomical segmentation. CNN-based models have been used to delineate brain tissues, subcortical nuclei, lesions, and tumors with accuracy comparable to expert human raters \cite{dolz2018denseunet}. For instance, networks trained on high-resolution T1-weighted images can reliably segment regions like the hippocampus or thalamus, which are critical in neurodegenerative disease research and surgical planning.

Image registration is another key domain of ML application. Recent methods such as VoxelMorph have demonstrated that deep networks can learn deformation fields for nonlinear registration in an unsupervised fashion, often achieving results comparable to classical algorithms while dramatically reducing computation time \cite{balakrishnan2019voxelmorph}. Registration accuracy, however, remains sensitive to contrast, anatomical variability, and initialization, all of which impact downstream tasks such as morphometry and surgical planning.

ML is also being used to localize brain targets that are difficult to visualize directly. In the context of deep brain stimulation (DBS), for example, researchers have trained models to identify target locations (e.g., subthalamic nucleus, globus pallidus internus) based on multimodal features or indirect anatomical markers \cite{yang2020deep}. These approaches promise to improve standardization and reduce inter-rater variability in surgical targeting, though they often depend on well-curated training datasets and may be vulnerable to domain shift.